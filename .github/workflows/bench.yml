name: Benchmarks

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

env:
  CARGO_TERM_COLOR: always

jobs:
  benchmark:
    name: Performance Benchmarks
    runs-on: ubuntu-latest

    steps:
    - name: Checkout sources
      uses: actions/checkout@v4
      with:
        fetch-depth: 0

    - name: Install stable toolchain
      uses: dtolnay/rust-toolchain@stable

    - name: Cache cargo
      uses: actions/cache@v4
      with:
        path: |
          ~/.cargo/bin/
          ~/.cargo/registry/index/
          ~/.cargo/registry/cache/
          ~/.cargo/git/db/
          target/
        key: ${{ runner.os }}-cargo-${{ hashFiles('**/Cargo.lock') }}
        restore-keys: ${{ runner.os }}-cargo-

    - name: Checkout main branch for comparison
      if: github.event_name == 'pull_request'
      uses: actions/checkout@v4
      with:
        ref: ${{ github.event.pull_request.base.ref }}
        path: main-branch
        fetch-depth: 0

    - name: Run main branch benchmarks
      if: github.event_name == 'pull_request'
      run: |
        echo "Running main branch benchmarks for comparison..."
        cd main-branch
        cargo bench --verbose
        mv target/criterion ../main-benchmark-results/
        cd ..

    - name: Run benchmarks
      run: |
        echo "Running performance benchmarks..."
        cargo bench --verbose

    - name: Parse benchmark results
      id: parse-results
      run: |
        echo "Parsing benchmark results for regression detection..."

        # Extract key metrics from criterion output
        python3 << 'EOF'
        import re
        import json
        import sys
        import os

        # Read the most recent benchmark output
        results = {}

        # Look for benchmark results in target/criterion directory
        if os.path.exists('target/criterion'):
            for bench_dir in os.listdir('target/criterion'):
                bench_path = os.path.join('target/criterion', bench_dir)
                if os.path.isdir(bench_path):
                    json_file = os.path.join(bench_path, 'new', 'estimates.json')
                    if os.path.exists(json_file):
                        try:
                            with open(json_file, 'r') as f:
                                data = json.load(f)
                                median = data.get('median', {}).get('point_estimate', 0)
                                results[bench_dir] = {
                                    'median_ns': median,
                                    'median_ms': median / 1_000_000.0
                                }
                        except:
                            pass

        # Save parsed results
        with open('parsed-results.json', 'w') as f:
            json.dump(results, f, indent=2)

        print(f"Found {len(results)} benchmarks")
        for name, metrics in results.items():
            print(f"{name}: {metrics['median_ms']:.3f}ms")

        EOF

    - name: Compare with main branch (PR only)
      if: github.event_name == 'pull_request'
      id: compare-results
      run: |
        echo "Comparing with main branch benchmarks..."

        python3 << 'EOF'
        import json
        import sys
        import os

        try:
            # Load current results
            with open('parsed-results.json', 'r') as f:
                current = json.load(f)

            # Load main branch results if available
            main_results = {}
            if os.path.exists('main-benchmark-results'):
                for bench_dir in os.listdir('main-benchmark-results'):
                    bench_path = os.path.join('main-benchmark-results', bench_dir)
                    if os.path.isdir(bench_path):
                        json_file = os.path.join(bench_path, 'new', 'estimates.json')
                        if os.path.exists(json_file):
                            try:
                                with open(json_file, 'r') as f:
                                    data = json.load(f)
                                    median = data.get('median', {}).get('point_estimate', 0)
                                    main_results[bench_dir] = {
                                        'median_ns': median,
                                        'median_ms': median / 1_000_000.0
                                    }
                            except:
                                pass

            print(f"Current results: {len(current)} benchmarks")
            print(f"Main branch results: {len(main_results)} benchmarks")

            # Compare results
            regressions = []
            improvements = []
            unchanged = []

            for name, current_metrics in current.items():
                if name in main_results:
                    main_metrics = main_results[name]
                    current_ms = current_metrics['median_ms']
                    main_ms = main_metrics['median_ms']

                    if main_ms > 0:
                        change_pct = ((current_ms - main_ms) / main_ms) * 100

                        if change_pct > 5.0:  # 5% regression threshold
                            regressions.append(f"{name}: {current_ms:.3f}ms vs {main_ms:.3f}ms ({change_pct:+.1f}%)")
                        elif change_pct < -5.0:  # 5% improvement threshold
                            improvements.append(f"{name}: {current_ms:.3f}ms vs {main_ms:.3f}ms ({change_pct:+.1f}%)")
                        else:
                            unchanged.append(f"{name}: {current_ms:.3f}ms vs {main_ms:.3f}ms ({change_pct:+.1f}%)")
                    else:
                        unchanged.append(f"{name}: {current_ms:.3f}ms (main was 0)")
                else:
                    unchanged.append(f"{name}: {current_ms:.3f}ms (new benchmark)")

            print(f"\nðŸ“Š Benchmark Comparison Results:")
            if regressions:
                print("ðŸ”´ REGRESSIONS DETECTED:")
                for regression in regressions:
                    print(f"  âš ï¸  {regression}")
                print(f"\nâŒ PR blocked due to performance regressions")
                sys.exit(1)

            if improvements:
                print("ðŸŸ¢ PERFORMANCE IMPROVEMENTS:")
                for improvement in improvements:
                    print(f"  âœ…  {improvement}")

            if unchanged:
                print("âœ… No significant performance changes")

            # Save comparison results
            comparison = {
                'regressions': regressions,
                'improvements': improvements,
                'unchanged': unchanged
            }

            with open('comparison-results.json', 'w') as f:
                json.dump(comparison, f, indent=2)

        except Exception as e:
            print(f"Error comparing results: {e}")
            print("Continuing without comparison...")

        EOF

    - name: Upload benchmark results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: benchmark-results
        path: |
          parsed-results.json
          comparison-results.json
          target/criterion/

    - name: Comment on PR with results
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v7
      with:
        script: |
          const fs = require('fs');

          // Read comparison results
          if (fs.existsSync('comparison-results.json')) {
            const comparison = JSON.parse(fs.readFileSync('comparison-results.json', 'utf8'));

            let commentBody = "## ðŸ“Š Benchmark Comparison Results\n\n";

            if (comparison.regressions.length > 0) {
              commentBody += "ðŸ”´ **Performance Regressions Detected:**\n\n";
              for (const regression of comparison.regressions) {
                commentBody += `âš ï¸ ${regression}\n`;
              }
              commentBody += "\nThis PR introduces performance regressions. Please review before merging.\n";
            }

            if (comparison.improvements.length > 0) {
              commentBody += "\nðŸŸ¢ **Performance Improvements:**\n\n";
              for (const improvement of comparison.improvements) {
                commentBody += `âœ… ${improvement}\n`;
              }
            }

            if (comparison.unchanged.length > 0) {
              commentBody += "\nâœ… **No Significant Performance Changes:**\n\n";
              for (const unchanged of comparison.unchanged) {
                commentBody += `  ${unchanged}\n`;
              }
            }

            commentBody += `\n\n*Comparison made against main branch (${context.sha})*`;
            commentBody += `\n*${comparison.regressions.length} regressions, ${comparison.improvements.length} improvements, ${comparison.unchanged.length} unchanged*`;

            // Post comment to PR
            await github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: commentBody
            });

            console.log("Benchmark comparison comment posted to PR");
          }
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
